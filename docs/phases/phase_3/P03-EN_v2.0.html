<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>F.A.R.F.A.N. | Phase 3: Micro-Result Scoring</title>
    <style>
        :root {
            --atroz-red-500: #C41E3A; --atroz-blue-electric: #00D4FF; --atroz-green-toxic: #39FF14;
            --atroz-copper-500: #B2642E; --atroz-copper-oxide: #17A589; --atroz-ink: #E5E7EB; --atroz-bg: #0A0A0A;
            --font-main: 'JetBrains Mono', monospace;
        }
        @font-face {
            font-family: 'JetBrains Mono';
            src: url('https://cdn.jsdelivr.net/gh/JetBrains/JetBrainsMono/web/woff2/JetBrainsMono-Regular.woff2') format('woff2');
            font-weight: 400; font-style: normal;
        }
        body { background-color: var(--atroz-bg); color: var(--atroz-ink); font-family: var(--font-main); margin: 0; padding: 0; font-size: 14px; line-height: 1.8; }
        .container { max-width: 960px; margin: 0 auto; padding: 40px; }
        h1, h2, h3 { font-weight: 400; letter-spacing: 2px; text-transform: uppercase; }
        h1 { font-size: 28px; color: var(--atroz-blue-electric); border-bottom: 1px solid var(--atroz-copper-500); padding-bottom: 15px; margin-bottom: 30px; }
        h2 { font-size: 18px; color: var(--atroz-copper-oxide); margin-top: 50px; border-left: 3px solid var(--atroz-copper-oxide); padding-left: 10px; }
        h3 { font-size: 16px; color: var(--atroz-copper-500); margin-top: 30px; }
        p, li { text-align: justify; text-shadow: 0 0 2px rgba(229, 231, 235, 0.1); }
        ul, ol { padding-left: 20px; }
        li { margin-bottom: 10px; }
        code { background-color: #1a1a1a; padding: 3px 6px; border-radius: 4px; font-size: 12px; color: var(--atroz-blue-electric); }
        .diagram-container { margin: 50px 0; padding: 30px; background: radial-gradient(circle, rgba(0, 212, 255, 0.1) 0%, transparent 70%); border: 1px solid var(--atroz-blue-electric); border-radius: 8px; }
    </style>
</head>
<body>
    <div class="container">
        <h1>Phase 3: Quantitative Scoring of Micro-Question Evidence</h1>

        <h2>1. Purpose</h2>
        <p>
            The primary function of Phase 3 is to translate the qualitative, raw evidence gathered in Phase 2 into a quantitative, normalized score. This phase systematically processes each <code>MicroQuestionRun</code> object, applying a specific scoring modality to its evidence payload. The modality, defined contractually in the monolith for each question, dictates the precise heuristic or algorithm used for quantification. The outcome is a standardized <code>ScoredMicroQuestion</code> object, which includes a raw score, a normalized score (typically 0.0 to 1.0), and a qualitative performance band. This act of quantification is the critical bridge between raw data extraction and the hierarchical aggregation that begins in Phase 4.
        </p>

        <h2>2. Key Definitions</h2>
        <ul>
            <li><strong>MicroQuestionScorer:</strong> A dedicated class from the <code>scoring.py</code> module that encapsulates the logic for all scoring modalities. It is the central processing engine of this phase.</li>
            <li><strong>Scoring Modality:</strong> One of six predefined methods (<code>TYPE_A</code> through <code>TYPE_F</code>) for calculating a score from an evidence object. Each type represents a different analytical heuristic, such as counting evidence elements, averaging confidence scores, or evaluating semantic similarity.</li>
            <li><strong>ScoringEvidence:</strong> A structured data object that adapts the raw <code>Evidence</code> from Phase 2 into a format suitable for the scoring functions, containing fields like <code>elements_found</code> and <code>confidence_scores</code>.</li>
            <li><strong>ScoredMicroQuestion:</strong> The final output artifact for each micro-question. It contains the original metadata and evidence, augmented with the newly calculated raw score, normalized score, and quality level (e.g., "Sufficient," "Insufficient").</li>
        </ul>

        <h2>3. Step-by-Step Operational Flow</h2>
        <p>
            This phase, managed by <code>Orchestrator._score_micro_results_async()</code>, operates as a parallel processing pipeline, applying a transformation to each result from the previous phase.
        </p>
        <ol>
            <li><strong>Asynchronous Task Distribution:</strong> Similar to Phase 2, the system processes the ~300 <code>MicroQuestionRun</code> objects concurrently. An asynchronous task is created for each object.</li>
            <li><strong>Evidence and Modality Extraction:</strong> Within each task, the system first inspects the input object. It extracts the raw evidence payload and, crucially, retrieves the <code>scoring_modality</code> from the object's metadata, which was originally sourced from the monolith.</li>
            <li><strong>Handling of Missing Evidence:</strong> If the input object contains an error or has no evidence payload, the scoring process is bypassed. A <code>ScoredMicroQuestion</code> is immediately generated with null scores and an appropriate error flag.</li>
            <li><strong>Scoring Logic Dispatch:</strong> The core of the phase occurs here. The system instantiates the <code>MicroQuestionScorer</code> and calls its <code>apply_scoring_modality</code> method. This method acts as a dispatcher; based on the provided modality (e.g., <code>TYPE_A</code>), it invokes the corresponding private method (e.g., <code>_score_modality_A</code>) to perform the calculation.
                <ul>
                    <li><code>TYPE_A</code> might calculate a score based on the sheer number of evidence elements found.</li>
                    <li><code>TYPE_B</code> might average the confidence scores associated with each element.</li>
                    <li>Other types may involve more complex logic based on semantic similarity or pattern matching results.</li>
                </ul>
            </li>
            <li><strong>Result Standardization and Encapsulation:</strong> The scoring method returns a structured result containing the raw and normalized scores. This is then used to construct the final <code>ScoredMicroQuestion</code> object, which is the output of the task.</li>
            <li><strong>Result Collection:</strong> The orchestrator gathers the ~300 <code>ScoredMicroQuestion</code> objects from the completed asynchronous tasks, forming the complete input for Phase 4.</li>
        </ol>

        <h2>4. System Diagram: Modality-Based Scoring Dispatch</h2>
        <div class="diagram-container">
            <svg width="100%" viewBox="0 0 900 650" xmlns="http://www.w3.org/2000/svg" font-family="JetBrains Mono, monospace" font-size="12px">
                <defs>
                    <linearGradient id="grad-blue" x1="0%" y1="0%" x2="100%" y2="100%"><stop offset="0%" stop-color="#00D4FF"/><stop offset="100%" stop-color="#04101A"/></linearGradient>
                    <linearGradient id="grad-green" x1="0%" y1="0%" x2="100%" y2="100%"><stop offset="0%" stop-color="#39FF14"/><stop offset="100%" stop-color="#0B231B"/></linearGradient>
                    <marker id="arrow-copper" viewBox="0 0 10 10" refX="5" refY="5" markerWidth="6" markerHeight="6" orient="auto-start-reverse"><path d="M 0 0 L 10 5 L 0 10 z" fill="#B2642E"/></marker>
                </defs>
                <style>
                    .box { stroke-width: 1.5; rx: 4px; }
                    .box-blue { fill: url(#grad-blue); stroke: #00D4FF; }
                    .box-green { fill: url(#grad-green); stroke: #39FF14; }
                    .label { fill: #E5E7EB; text-anchor: middle; }
                    .sub-label { fill: #E5E7EB; opacity: 0.7; font-size: 10px; text-anchor: middle; }
                    .flow-line { stroke: #B2642E; stroke-width: 1.5; marker-end: url(#arrow-copper); }
                    .dispatch-box { fill: rgba(178, 100, 46, 0.1); stroke: var(--atroz-copper-500); stroke-dasharray: 4,4; rx: 8px; }
                </style>

                <rect x="325" y="20" width="250" height="60" class="box box-green"/>
                <text x="450" y="50" class="label">Input: MicroQuestionRun</text>
                <text x="450" y="65" class="sub-label">(from Phase 2, with raw Evidence)</text>

                <path class="flow-line" d="M450 80 V 120"/>

                <rect x="350" y="120" width="200" height="60" class="box box-blue"/>
                <text x="450" y="155" class="label">MicroQuestionScorer</text>

                <path class="flow-line" d="M450 180 V 220"/>
                <text x="500" y="205" class="sub-label" fill="#B2642E">Dispatch based on Modality</text>

                <g id="dispatch-logic">
                    <rect x="50" y="220" width="800" height="250" class="dispatch-box"/>
                    <text x="450" y="245" class="label" fill="#B2642E">Scoring Modality Logic (scoring.py)</text>

                    <rect x="80" y="280" width="100" height="50" class="box box-blue" opacity="0.8"/>
                    <text x="130" y="310" class="label">TYPE_A</text>
                    <rect x="200" y="280" width="100" height="50" class="box box-blue" opacity="0.8"/>
                    <text x="250" y="310" class="label">TYPE_B</text>
                    <rect x="320" y="280" width="100" height="50" class="box box-blue" opacity="0.8"/>
                    <text x="370" y="310" class="label">TYPE_C</text>
                    <rect x="480" y="280" width="100" height="50" class="box box-blue" opacity="0.8"/>
                    <text x="530" y="310" class="label">TYPE_D</text>
                    <rect x="600" y="280" width="100" height="50" class="box box-blue" opacity="0.8"/>
                    <text x="650" y="310" class="label">TYPE_E</text>
                    <rect x="720" y="280" width="100" height="50" class="box box-blue" opacity="0.8"/>
                    <text x="770" y="310" class="label">TYPE_F</text>

                    <text x="130" y="350" class="sub-label">Count-based</text>
                    <text x="250" y="350" class="sub-label">Confidence avg</text>
                    <text x="370" y="350" class="sub-label">Semantic sim</text>
                    <text x="530" y="350" class="sub-label">[Heuristic D]</text>
                    <text x="650" y="350" class="sub-label">[Heuristic E]</text>
                    <text x="770" y="350" class="sub-label">[Heuristic F]</text>

                    <path class="flow-line" d="M130 380 V 440"/>
                    <path class="flow-line" d="M250 380 V 440"/>
                    <path class="flow-line" d="M370 380 V 440"/>
                    <path class="flow-line" d="M530 380 V 440"/>
                    <path class="flow-line" d="M650 380 V 440"/>
                    <path class="flow-line" d="M770 380 V 440"/>
                </g>

                <path class="flow-line" d="M130 470 H 450"/>
                <path class="flow-line" d="M250 470 H 450"/>
                <path class="flow-line" d="M370 470 H 450"/>
                <path class="flow-line" d="M770 470 H 450"/>
                <path class="flow-line" d="M650 470 H 450"/>
                <path class="flow-line" d="M530 470 H 450"/>

                <text x="450" y="500" class="sub-label" fill="#B2642E">Combine and Standardize</text>
                <path class="flow-line" d="M450 470 v -10"/>
                <path class="flow-line" d="M450 510 v 20"/>


                <rect x="325" y="560" width="250" height="60" class="box box-green"/>
                <text x="450" y="590" class="label">Output: ScoredMicroQuestion</text>
                <text x="450" y="605" class="sub-label">(with normalized score)</text>

            </svg>
        </div>

        <h2>5. Illustrative Pseudocode</h2>
        <pre><code>
ASYNC FUNCTION _score_micro_results_async(micro_results, config):
    tasks = []
    FOR item IN micro_results:
        task = CREATE_TASK(score_single_item(item))
        tasks.APPEND(task)

    scored_results = AWAIT asyncio.gather(*tasks)
    RETURN scored_results

ASYNC FUNCTION score_single_item(item: MicroQuestionRun):
    // Step 2: Extract modality and evidence
    modality = item.metadata.scoring_modality
    evidence = item.evidence

    // Step 3: Handle cases with no evidence
    IF item.error OR NOT evidence:
        RETURN new ScoredMicroQuestion(..., score=None, error="missing_evidence")

    // Step 4: Dispatch to the appropriate scoring logic
    scorer = new MicroQuestionScorer()
    scored_data = AWAIT IN_THREAD(
        scorer.apply_scoring_modality(modality, evidence)
    )

    // Step 5: Encapsulate the final scored result
    RETURN new ScoredMicroQuestion(
        ...,
        score = scored_data.raw_score,
        normalized_score = scored_data.normalized_score,
        quality_level = scored_data.quality_level
    )
        </code></pre>

        <h2>6. Operational Checklist for Auditing</h2>
        <ul>
            <li><strong>Verify Input Integrity:</strong> Confirm the list of <code>MicroQuestionRun</code> objects from Phase 2 is passed correctly.</li>
            <li><strong>Check Output Integrity:</strong> Ensure the output is a list of <code>ScoredMicroQuestion</code> objects of the same length as the input list.</li>
            <li><strong>Audit Score Normalization:</strong> For a sample of results, verify that the <code>normalized_score</code> is a float value, typically between 0.0 and 1.0.</li>
            <li><strong>Validate Modality Dispatch:</strong> Select a question from the monolith, note its `scoring_modality`, and trace the code path to confirm that the correct `_score_modality_X` method within `MicroQuestionScorer` is being invoked for that question.</li>
            <li><strong>Examine Error Handling:</strong> Check items that had errors in Phase 2. Confirm that they are propagated correctly with null scores in this phase's output.</li>
        </ul>
    </div>
</body>
</html>
