<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>F.A.R.F.A.N. - Phase 1: Document Ingestion</title>
    <style>
        :root {
            --atroz-red-500: #C41E3A;
            --atroz-blue-electric: #00D4FF;
            --atroz-green-toxic: #39FF14;
            --atroz-copper-500: #B2642E;
            --atroz-copper-oxide: #17A589;
            --atroz-ink: #E5E7EB;
            --atroz-bg: #0A0A0A;
            --font-main: 'JetBrains Mono', monospace;
        }

        @font-face {
            font-family: 'JetBrains Mono';
            src: url('https://cdn.jsdelivr.net/gh/JetBrains/JetBrainsMono/web/eot/JetBrainsMono-Regular.eot') format('embedded-opentype'),
                 url('https://cdn.jsdelivr.net/gh/JetBrains/JetBrainsMono/web/woff2/JetBrainsMono-Regular.woff2') format('woff2'),
                 url('https://cdn.jsdelivr.net/gh/JetBrains/JetBrainsMono/web/woff/JetBrainsMono-Regular.woff') format('woff');
            font-weight: 400;
            font-style: normal;
        }

        body {
            background-color: var(--atroz-bg);
            color: var(--atroz-ink);
            font-family: var(--font-main);
            margin: 0;
            padding: 0;
            font-size: 12px;
            line-height: 1.7;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 40px;
        }

        h1, h2, h3 {
            font-weight: 400;
            letter-spacing: 3px;
            text-transform: uppercase;
        }

        h1 {
            font-size: 24px;
            color: var(--atroz-blue-electric);
            border-bottom: 1px solid var(--atroz-copper-500);
            padding-bottom: 10px;
        }

        h2 {
            font-size: 16px;
            color: var(--atroz-ink);
            margin-top: 40px;
        }

        h3 {
            font-size: 14px;
            color: var(--atroz-copper-500);
        }

        p {
            margin-bottom: 20px;
            text-align: justify;
        }

        .meta-info {
            background-color: rgba(0, 212, 255, 0.05);
            border-left: 3px solid var(--atroz-blue-electric);
            padding: 15px;
            margin: 20px 0;
            font-size: 11px;
        }

        .meta-info strong {
            color: var(--atroz-copper-500);
            text-transform: uppercase;
        }

        .diagram-container {
            margin: 40px 0;
            text-align: center;
            background: radial-gradient(circle, rgba(4,16,26,0.5) 0%, transparent 70%);
            padding: 20px;
        }

        code {
            background-color: #1a1a1a;
            padding: 2px 5px;
            border-radius: 3px;
            font-size: 11px;
            color: var(--atroz-blue-electric);
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Phase 1: The Corpus Dissection</h1>

        <div class="meta-info">
            <strong>Pipeline Position:</strong> Blue (Structuring & Processing)<br>
            <strong>Primary Contract:</strong> P01-ES v1.0 Validation Gates<br>
            <strong>Evidence Depth:</strong> Thematized Smart Policy Chunks
        </div>

        <h2>The Wound: The Monolithic, Opaque Document</h2>
        <p>
            Raw policy documents, typically in PDF format, represent a state of high-entropy information. They are monolithic, unstructured, and opaque to direct machine analysis. Attempting to apply complex analytical queries to such a document is akin to performing surgery with a sledgehammerâ€”crude, imprecise, and destructive of the nuanced information held within. The core problem is the lack of semantic addressability; the document is a single, undifferentiated mass. To unlock its value, it must be systematically dissected into coherent, thematically-tagged fragments that the subsequent analytical phases can target with surgical precision.
        </p>

        <h2>The Instrument: The Smart Policy Chunking Pipeline</h2>
        <p>
            The <code>_ingest_document</code> method serves as the entry point to a sophisticated surgical procedure: the <code>CPPIngestionPipeline</code>. This is where the raw corpus is transformed from a single block into a highly structured and validated data artifact. The pipeline's primary function is to produce exactly 60 "Smart Policy Chunks" (SPCs). This number is not arbitrary; it is a strict contractual requirement of the <strong>P01-ES v1.0 specification</strong>, representing the intersection of 10 distinct policy areas and 6 analytical dimensions. The pipeline is therefore not merely chunking text; it is performing a thematically-driven dissection, ensuring each resulting fragment is imbued with a specific, addressable identity.
        </p>
        <p>
            Crucially, this phase operates under a series of unforgiving validation gates. The orchestrator verifies that the final chunk count is precisely 60. It then iterates through every single chunk to ensure the mandatory presence of <code>policy_area_id</code> and <code>dimension_id</code> metadata tags. The absence of these tags, or a deviation from the exact chunk count, constitutes a fatal breach of contract and results in an immediate pipeline abortion. This rigid enforcement ensures that the output is not just a collection of text snippets, but a perfectly formed, 60-part matrix of evidence, ready for the multi-threaded micro-question analysis in Phase 2.
        </p>

        <div class="diagram-container">
            <svg width="800" height="300" viewBox="0 0 800 300" xmlns="http://www.w3.org/2000/svg" font-family="JetBrains Mono, monospace" font-size="12px">
                <defs>
                    <linearGradient id="redHex" x1="0%" y1="0%" x2="100%" y2="100%"><stop offset="0%" style="stop-color:#C41E3A;" /><stop offset="100%" style="stop-color:#3A0E0E;" /></linearGradient>
                    <linearGradient id="blueHex" x1="0%" y1="0%" x2="100%" y2="100%"><stop offset="0%" style="stop-color:#00D4FF;" /><stop offset="100%" style="stop-color:#04101A;" /></linearGradient>
                    <linearGradient id="greenHex" x1="0%" y1="0%" x2="100%" y2="100%"><stop offset="0%" style="stop-color:#39FF14;" /><stop offset="100%" style="stop-color:#0B231B;" /></linearGradient>
                    <marker id="arrow" viewBox="0 0 10 10" refX="5" refY="5" markerWidth="6" markerHeight="6" orient="auto-start-reverse"><path d="M 0 0 L 10 5 L 0 10 z" fill="#B2642E" /></marker>
                </defs>

                <style>
                    .label { fill: #E5E7EB; text-anchor: middle; }
                    .sub-label { fill: #E5E7EB; opacity: 0.7; font-size: 9px; text-anchor: middle; }
                    .connector { stroke: #B2642E; stroke-width: 1.5; marker-end: url(#arrow); }
                    .gate-label { fill: #C41E3A; font-size: 10px; font-weight: bold; text-anchor: middle; text-transform:uppercase; }
                </style>

                <!-- Input Document -->
                <rect x="50" y="100" width="100" height="100" fill="url(#redHex)" stroke="#C41E3A" stroke-width="1.5" />
                <text x="100" y="145" class="label">RAW PDF</text>
                <text x="100" y="160" class="sub-label">Monolithic Corpus</text>

                <!-- Processing Pipeline -->
                <rect x="250" y="125" width="200" height="50" fill="url(#blueHex)" stroke="#00D4FF" stroke-width="1.5" />
                <text x="350" y="153" class="label">CPPIngestionPipeline</text>

                <!-- Validation Gates -->
                <path d="M480 100 L480 200" stroke="#C41E3A" stroke-width="2" stroke-dasharray="4,4" />
                <text x="495" y="95" class="gate-label" transform="rotate(90, 495, 95)">P01-ES v1.0 GATES</text>
                <text x="520" y="145" class="label" fill="#E5E7EB" text-anchor="start">1. Chunk Count == 60</text>
                <text x="520" y="165" class="label" fill="#E5E7EB" text-anchor="start">2. All Chunks Tagged</text>


                <!-- Output Chunks -->
                <g transform="translate(650, 110)">
                    <rect x="0" y="0" width="40" height="40" fill="url(#greenHex)" stroke="#39FF14" stroke-width="1" />
                    <text x="20" y="25" class="label" font-size="9px">SPC-1</text>
                </g>
                <g transform="translate(700, 110)">
                    <rect x="0" y="0" width="40" height="40" fill="url(#greenHex)" stroke="#39FF14" stroke-width="1" />
                    <text x="20" y="25" class="label" font-size="9px">SPC-2</text>
                </g>
                 <g transform="translate(650, 160)">
                    <rect x="0" y="0" width="40" height="40" fill="url(#greenHex)" stroke="#39FF14" stroke-width="1" />
                     <text x="20" y="25" class="label" font-size="9px">...</text>
                </g>
                <g transform="translate(700, 160)">
                    <rect x="0" y="0" width="40" height="40" fill="url(#greenHex)" stroke="#39FF14" stroke-width="1" />
                    <text x="20" y="25" class="label" font-size="9px">SPC-60</text>
                </g>
                <text x="695" y="225" class="sub-label">Validated & Thematized Chunks</text>


                <!-- Connectors -->
                <line class="connector" x1="150" y1="150" x2="250" y2="150" />
                <line class="connector" x1="450" y1="150" x2="470" y2="150" />
                <line class="connector" x1="510" y1="150" x2="650" y2="150" />
            </svg>
        </div>

        <h2>The Scar: The Preprocessed Document Artifact</h2>
        <p>
            The successful output of this phase is the <code>PreprocessedDocument</code> object. This is a profound transformation of the original data. It is no longer a single entity but a structured container holding the 60 validated SPCs, the full extracted text, sentence boundaries, and critical metadata about the ingestion process itself. This artifact represents the flesh of the document, surgically prepared and laid out for fine-grained analysis. Its existence signifies that the riskiest, most ambiguous part of the data preparation process is complete. The system now has a clean, predictable, and semantically-rich substrate upon which to build its entire analytical edifice.
        </p>

        <h3>Provenance Trail</h3>
        <p>
            The execution of this phase is governed by the following key components:
            <ul>
                <li><code>Orchestrator._ingest_document()</code>: The orchestrator method that triggers and validates the ingestion process.</li>
                <li><code>farfan_pipeline.processing.spc_ingestion.CPPIngestionPipeline</code>: The dedicated, external pipeline responsible for the canonical dissection of the document.</li>
                <li><code>PreprocessedDocument.ensure()</code>: The adapter method that formally transforms the output of the ingestion pipeline into the orchestrator's native data structure.</li>
                <li><code>P01_EXPECTED_CHUNK_COUNT</code>: The hard-coded constant (60) that forms the core of the P01-ES v1.0 validation gate, ensuring a deterministic output structure.</li>
            </ul>
        </p>
    </div>
</body>
</html>
